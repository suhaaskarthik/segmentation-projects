{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1352000,
          "sourceType": "datasetVersion",
          "datasetId": 786903
        },
        {
          "sourceId": 10960308,
          "sourceType": "datasetVersion",
          "datasetId": 6818713
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "humanparser-segmenation",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhaaskarthik/segmentation-projects/blob/main/humanparser_segmenation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "8GhGfxZNxd6d"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "soumikrakshit_human_segmentation_path = kagglehub.dataset_download('soumikrakshit/human-segmentation')\n",
        "suhaaskarthikeyan_best_checkpoint_path = kagglehub.dataset_download('suhaaskarthikeyan/best-checkpoint')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "04w3Sdxvxd6f"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human body segmentation task\n",
        "In this notebook we build and train an AI model that can predict masks to segment humans from an image.The training data consists of images and their respective masks. These masks must be converted to pure black  and white format (1 and 0). After preprocessing the dataset, creating the training and testing datasets, we build our Unet model. This model is particularly useful for medical image segmentation, but can be used for a wide variety of segmentation tasks. We add data augmentation, learning rate schedulers(not used in this notebook), and checkpoint callbacks. After training the model we test with our very own custom images. This dataset consists of approx. 28,000 images which require faster GPUs with greater RAM, hence training should be done with a portion of the dataset. I've already trained this model(with full training dataset) using the A100 GPU provided by google colab pro. Hence the model has reached pretty great results. So please try to play around with this notebook and any suggestions and improvements are appreciated...\n"
      ],
      "metadata": {
        "id": "3LS8AzlWxd6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load 5000 training images, u can play around with this number (more training images, greater the accuracy), but you must account for RAM and GPU speed. We shuffle the dataset and split training and testing"
      ],
      "metadata": {
        "id": "kYbYOPkFxd6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "\n",
        "#you can select the number of training samples needed by changing from 5000\n",
        "files = os.listdir('/kaggle/input/human-segmentation/instance-level_human_parsing/instance-level_human_parsing/Training/Images')[:5000]\n",
        "files = [f'/kaggle/input/human-segmentation/instance-level_human_parsing/instance-level_human_parsing/Training/Images/{file}' for file in files]\n",
        "random.seed(123)\n",
        "random.shuffle(files)\n",
        "train_sample = int(len(files)*0.9)\n",
        "training_files = files[:train_sample]\n",
        "testing_files = files[train_sample:]"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:21:30.33944Z",
          "iopub.execute_input": "2025-03-08T12:21:30.33979Z",
          "iopub.status.idle": "2025-03-08T12:21:30.37567Z",
          "shell.execute_reply.started": "2025-03-08T12:21:30.339761Z",
          "shell.execute_reply": "2025-03-08T12:21:30.375061Z"
        },
        "id": "uRdNrrlZxd6h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the dataset, not directly, but through symbolic tensors. This would create a computational graph, which will be the executed during the training process. So loading the data and preprocessing steps will happen during model fitting. Here we wrap the preprocessing function around a py_function to ensure compatibility to numpy and other data formats."
      ],
      "metadata": {
        "id": "EOBuao6ixd6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def preprocess(file):\n",
        "    # Decode file path\n",
        "    path = file.numpy().decode(\"utf-8\")\n",
        "\n",
        "    # Load and process the real image\n",
        "    real_img = tf.io.read_file(path)\n",
        "    real_img = tf.image.decode_jpeg(real_img, channels=3)\n",
        "    real_img = tf.image.resize(real_img, (256, 256))\n",
        "    real_img = tf.cast(real_img, tf.float32) / 255.0  # Normalize to [0,1]\n",
        "\n",
        "    # Load and process the mask image\n",
        "    fileno = (path.split('/')[-1]).split('.')[0]\n",
        "    img_path = f'/kaggle/input/human-segmentation/instance-level_human_parsing/instance-level_human_parsing/Training/Human/{fileno}.png'\n",
        "\n",
        "    mask_img = tf.io.read_file(img_path)\n",
        "    mask_img = tf.image.decode_png(mask_img, channels=1)  # Read as grayscale\n",
        "    mask_img = tf.image.resize(mask_img, (256, 256))\n",
        "\n",
        "    # Normalize mask image and apply threshold\n",
        "    mask_img = tf.cast(mask_img, tf.float32) / 255.0  # Normalize to [0,1]\n",
        "    mask_img = tf.where(mask_img >0.0, 1.0, 0.0)  # Thresholding-  to convert grayscale to pure black and white\n",
        "\n",
        "    return real_img, mask_img\n",
        "\n",
        "def load_data(file_path):\n",
        "    img, mask = tf.py_function(\n",
        "        func=preprocess,\n",
        "        inp=[file_path],\n",
        "        Tout=[tf.float32, tf.float32]\n",
        "    )\n",
        "\n",
        "    # Explicitly set tensor shapes\n",
        "    img.set_shape((256, 256, 3))\n",
        "    mask.set_shape((256, 256, 1))\n",
        "\n",
        "    return img, mask"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:21:32.406544Z",
          "iopub.execute_input": "2025-03-08T12:21:32.406872Z",
          "iopub.status.idle": "2025-03-08T12:21:32.416088Z",
          "shell.execute_reply.started": "2025-03-08T12:21:32.406849Z",
          "shell.execute_reply": "2025-03-08T12:21:32.415155Z"
        },
        "id": "IVTaNjwGxd6i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load our training and testing dataset by converting the train and test files into a tensorflow dataset, we map them to the preprocessing function and batch them up. We do the same thing with the testing files"
      ],
      "metadata": {
        "id": "lr1G7h-ixd6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare training dataset\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices(training_files)\n",
        "training_dataset = training_dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "training_dataset = training_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Load and prepare testing dataset\n",
        "testing_dataset = tf.data.Dataset.from_tensor_slices(testing_files)\n",
        "testing_dataset = testing_dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "testing_dataset = testing_dataset.batch(64).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:21:34.629251Z",
          "iopub.execute_input": "2025-03-08T12:21:34.629529Z",
          "iopub.status.idle": "2025-03-08T12:21:34.712007Z",
          "shell.execute_reply.started": "2025-03-08T12:21:34.629507Z",
          "shell.execute_reply": "2025-03-08T12:21:34.711072Z"
        },
        "id": "xNz07QGXxd6j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a sample from our training dataset and visualise the images along with their masks"
      ],
      "metadata": {
        "id": "_NDxh0b3xd6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i,o in training_dataset.take(1):\n",
        "  for image, mask in zip(i, o):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    ax[0].imshow(tf.squeeze(mask), cmap=\"gray\")\n",
        "    ax[0].set_title(\"Grayscale Image\")\n",
        "    ax[0].axis(\"off\")\n",
        "    new_arr = tf.cast(image,dtype=tf.float32)\n",
        "    ax[1].imshow(new_arr)\n",
        "    ax[1].set_title(\"Black & White (Thresholded)\")\n",
        "    ax[1].axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:21:36.450932Z",
          "iopub.execute_input": "2025-03-08T12:21:36.451295Z",
          "iopub.status.idle": "2025-03-08T12:21:51.177376Z",
          "shell.execute_reply.started": "2025-03-08T12:21:36.451258Z",
          "shell.execute_reply": "2025-03-08T12:21:51.176571Z"
        },
        "id": "enYxIQnTxd6k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "import tensorflow.keras.backend as K"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:22:18.117196Z",
          "iopub.execute_input": "2025-03-08T12:22:18.117506Z",
          "iopub.status.idle": "2025-03-08T12:22:18.12205Z",
          "shell.execute_reply.started": "2025-03-08T12:22:18.117485Z",
          "shell.execute_reply": "2025-03-08T12:22:18.121133Z"
        },
        "id": "xtWaV568xd6k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation steps"
      ],
      "metadata": {
        "id": "j36qJOAxxd6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),    # Random horizontal flip\n",
        "    tf.keras.layers.RandomFlip(\"vertical\"),      # Random vertical flip\n",
        "    tf.keras.layers.RandomRotation(0.2),         # Random rotation (between -0.2 and 0.2 radians)\n",
        "    tf.keras.layers.RandomZoom(0.2),             # Random zoom (between 0.8 and 1.2 scale)\n",
        "    tf.keras.layers.RandomContrast(0.2),         # Random contrast adjustment\n",
        "    tf.keras.layers.RandomBrightness(0.2),       # Random brightness adjustment\n",
        "    tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2),  # Random translation\n",
        "    tf.keras.layers.RandomHeight(0.2),           # Random height change\n",
        "    tf.keras.layers.RandomWidth(0.2),            # Random width change\n",
        "])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:22:20.037254Z",
          "iopub.execute_input": "2025-03-08T12:22:20.037568Z",
          "iopub.status.idle": "2025-03-08T12:22:20.063879Z",
          "shell.execute_reply.started": "2025-03-08T12:22:20.03754Z",
          "shell.execute_reply": "2025-03-08T12:22:20.063192Z"
        },
        "id": "bdkY9Hcaxd6k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create our Unet model, which cosists of an encoder and decoder along with a bottle-neck. The encoder downsamples with convolutional layers, whereas the decoder upsamples with conv2d layers. The bottle-neck mediates the process. Final we add concatenation layers (residual layers) to ensure a good flow of information across the complex architecture. Here we use a different kind of loss function called dice loss, that measures similairites between the predicted and actual mask. Model compilation process is also done here"
      ],
      "metadata": {
        "id": "E3wDGgKDxd6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Dice Loss Function\n",
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coefficient(y_true, y_pred)\n",
        "\n",
        "# U-Net Model\n",
        "def unet(input_size=(256, 256, 3)):\n",
        "    inputs = Input(input_size)\n",
        "    inputs = data_augmentation(inputs)\n",
        "    # Encoder (Contracting Path)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = MaxPooling2D(pool_size=(2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = MaxPooling2D(pool_size=(2, 2))(c2)\n",
        "\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = MaxPooling2D(pool_size=(2, 2))(c3)\n",
        "\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "    # Bottleneck\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    # Decoder (Expanding Path)\n",
        "    u6 = UpSampling2D(size=(2, 2))(c5)\n",
        "    u6 = concatenate([u6, c4])  # Skip connection\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u7 = UpSampling2D(size=(2, 2))(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u8 = UpSampling2D(size=(2, 2))(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    u9 = UpSampling2D(size=(2, 2))(c8)\n",
        "    u9 = concatenate([u9, c1])\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)  # Binary Segmentation Output\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Compile Model\n",
        "model = unet()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss=[dice_loss, BinaryCrossentropy()],\n",
        "              metrics=[dice_coefficient])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:22:22.153009Z",
          "iopub.execute_input": "2025-03-08T12:22:22.153338Z",
          "iopub.status.idle": "2025-03-08T12:22:22.3151Z",
          "shell.execute_reply.started": "2025-03-08T12:22:22.15331Z",
          "shell.execute_reply": "2025-03-08T12:22:22.314252Z"
        },
        "id": "IVQ15KZJxd6l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block trains the model and adds a checkpoint to save the model for each epoch. I trained this model in google colab with full dataset(15 epochs) and A100 gpu that took almost an hour and yielded a dice loss of around 0.11, this is a pretty great score!!"
      ],
      "metadata": {
        "id": "nIoivftKxd6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the checkpoint callback\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=\"model_checkpoint_epoch_{epoch:02d}.keras\",  # Save model after each epoch\n",
        "    save_weights_only=False,  # Set to True if you only want to save weights\n",
        "    save_best_only=False,  # Set to True to save only the best model based on validation loss\n",
        "    verbose=1\n",
        ")\n",
        "#model.fit(training_dataset,validation_data = testing_dataset,epochs = 15, verbose = 1, batch_size = 64,callbacks =[checkpoint_callback])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:22:27.405039Z",
          "iopub.execute_input": "2025-03-08T12:22:27.405324Z",
          "iopub.status.idle": "2025-03-08T12:22:27.409242Z",
          "shell.execute_reply.started": "2025-03-08T12:22:27.405301Z",
          "shell.execute_reply": "2025-03-08T12:22:27.408186Z"
        },
        "id": "ZbrivZC8xd6l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the training is over we can test with custom images, or we can just use some image from testing dataset.We load the model with the latest checkpoint. When loading we must make sure we call the dice loss and dice coefficient functions again, like so."
      ],
      "metadata": {
        "id": "WjKgDo_Nxd6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1 - dice_coefficient(y_true, y_pred)\n",
        "model = load_model('/kaggle/input/best-checkpoint/model_checkpoint_epoch_13.keras',custom_objects={'dice_loss': dice_loss, 'dice_coefficient':dice_coefficient})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:22:29.351173Z",
          "iopub.execute_input": "2025-03-08T12:22:29.351458Z",
          "iopub.status.idle": "2025-03-08T12:22:30.651602Z",
          "shell.execute_reply.started": "2025-03-08T12:22:29.351436Z",
          "shell.execute_reply": "2025-03-08T12:22:30.650947Z"
        },
        "id": "X6fI_RD_xd6l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see,the output produced by the model, seems to be a bit messy here and there. But overall the model performs very well. It is able to perfectly capture faces and other body parts, but it is not able to perfectly capture some other finer details"
      ],
      "metadata": {
        "id": "AYaKOwGNxd6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def preprocess(path):\n",
        "    real_img = tf.io.read_file(path)\n",
        "    real_img = tf.image.decode_jpeg(real_img, channels=3)\n",
        "    real_img = tf.image.resize(real_img, (256, 256))\n",
        "    real_img = tf.cast(real_img, tf.float32) / 255.0\n",
        "    return real_img\n",
        "\n",
        "#go through the testing dataset\n",
        "for test_img in os.listdir('/kaggle/input/human-segmentation/instance-level_human_parsing/instance-level_human_parsing/Testing/Images')[:10]:\n",
        "    res = preprocess(f'/kaggle/input/human-segmentation/instance-level_human_parsing/instance-level_human_parsing/Testing/Images/{test_img}')\n",
        "    plt.imshow(res)\n",
        "    plt.show()\n",
        "    mask =model.predict(tf.expand_dims(res,axis = 0))\n",
        "    #mutiply mask with the actual image to segment the human beings present in the photo\n",
        "    plt.imshow(tf.squeeze(mask*res))\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-08T12:23:09.154368Z",
          "iopub.execute_input": "2025-03-08T12:23:09.154692Z",
          "iopub.status.idle": "2025-03-08T12:23:14.015212Z",
          "shell.execute_reply.started": "2025-03-08T12:23:09.154633Z",
          "shell.execute_reply": "2025-03-08T12:23:14.01431Z"
        },
        "id": "JQkv1-Eaxd6l"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}